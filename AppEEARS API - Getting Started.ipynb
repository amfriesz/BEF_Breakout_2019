{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1>Using the AρρEEARS API in your Analysis Workflow - Getting Started</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objective\n",
    "The intent of this tutorial is to familiarize [Earth Observation Data](https://earthdata.nasa.gov/earth-observation-data) users with the [AρρEEARS](https://lpdaac.usgs.gov/tools/appeears/) application programming interface (API) with demonstrations on how the API, and the services it provides, can be leveraged in an analysis workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Covered\n",
    "1. [**Getting Started**](#gettingstarted)  \n",
    "    1.1 [Enable Access to the API](#1.1)  \n",
    "    1.2 [Login](#1.2)  \n",
    "2. [**Submit an Area Request**](#submittask)  \n",
    "    2.1 [Import a Shapefile](#2.1)  \n",
    "    2.2 [Compile the JSON payload to submit to AρρEEARS](#2.2)  \n",
    "    2.3 [Submit a task request](#2.3)  \n",
    "    2.4 [Get task status](#2.4)  \n",
    "3. [**Download a Request [Bundle API]**](#downloadrequest)  \n",
    "    3.1 [List files associated with the request](#3.2)  \n",
    "    3.2 [Download files in a request](#3.2)  \n",
    "4. [**Explore AρρEEARS Outputs**](#explore)  \n",
    "    4.1 [Open and explore data using xarray](#4.1)  \n",
    "    4.2 [Create summary statistics](#4.2)  \n",
    "    4.3 [Create plots](#4.3)  \n",
    "5. [**Quality Filtering**](#qualityfiltering)  \n",
    "    5.1 [Decode quality values](#5.1)  \n",
    "    5.2 [Create and apply quality mask](#5.2)  \n",
    "    5.3 [Plot quality filtered data](#5.3)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AρρEEARS Information\n",
    "To access AρρEEARS, visit: https://lpdaacsvc.cr.usgs.gov/appeears/\n",
    "\n",
    "For comprehensive documentation of the full functionality of the [AρρEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/), please see the AρρEEARS API Documentation: https://lpdaacsvc.cr.usgs.gov/appeears/api/\n",
    "\n",
    "Throughout the exercise, specific sections of the API documentation can be accessed by clicking the hyperlinked text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies \n",
    "- This Python Jupyter Notebook tutorial was has be test on Python versions 3.6 and 3.7\n",
    "\n",
    "- Minicondas was used to create the python environments\n",
    "    - Windows OS  \n",
    "    `conda create -n py3.7 python=3.7`\n",
    "\n",
    "- Required Python packages were installed from the conda-forge channel. Installing packages from the conda-forge channel is done by adding conda-forge to your channels with: `conda config --add channels conda-forge`\n",
    "\n",
    "- Required Packages needed for this exercise are listed below. \n",
    "    - requests  \n",
    "    `conda install requests`  \n",
    "    - pandas  \n",
    "    `conda install pandas`  \n",
    "    - geopandas  \n",
    "    `conda install geopandas`  \n",
    "    - xarray  \n",
    "    `conda install xarray`  \n",
    "    - numpy  \n",
    "    `conda install numpy`  \n",
    "    - netCDF4  \n",
    "    `conda install netCDF4`  \n",
    "    - pyviz &emsp;&emsp;**NOTE** - [PyViz](http://pyviz.org/) is installed using the pyviz channel not conda-forge.  \n",
    "    `conda install -c pyviz hvplot`  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting Started <a id=\"gettingstarted\"></a>\n",
    "[AρρEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/) access requires the same [NASA Earthdata Login](https://urs.earthdata.nasa.gov/) as the AρρEEARS user interface. In addition to having a valid NASA Earthdata Login account, the API feature must be enabled for the user within AρρEEARS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Enable Access to the API <a id=\"1.1\"></a>\n",
    "> To enable access to the [AρρEEARS API](https://lpdaacsvc.cr.usgs.gov/appeears/api/), navigate to the [AρρEEARS website](https://lpdaacsvc.cr.usgs.gov/appeears/). Click the *Sign In* button in the top right portion of the AρρEEARS landing page screen.  \n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"https://lpdaacsvc.cr.usgs.gov/assets/images/help/image001.7f0d8820.png\" />\n",
    "</td></tr></table>  \n",
    "\n",
    "> Once you are signed in, click the *Manage User* icon in the top right portion of the AρρEEARS landing page screen and select *Settings*.   \n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"https://lpdaacsvc.cr.usgs.gov/assets/images/help/api/image001.3bb7c98a.png\" />\n",
    "</td></tr></table>  \n",
    "\n",
    "> Select the *Enable API* box to gain access to the AρρEEARS API.  \n",
    "\n",
    "<table><tr><td>\n",
    "    <img src=\"https://lpdaacsvc.cr.usgs.gov/assets/images/help/api/image002.ebbb9431.png\" />\n",
    "</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Login to AρρEEARS/Earthdata <a id=\"1.2\"></a>\n",
    "> To submit a request, you must first [login](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#login) to the AρρEEARS API using your Earthdata login credentials.  We’ll use the `getpass` package to conceal our Earthdata login username and password. When executed, the code below will prompt you to enter your username followed by your password and store them as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages\n",
    "import requests\n",
    "import getpass\n",
    "import time\n",
    "import os\n",
    "import cgi\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray\n",
    "import numpy as np\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter Earthdata login credentials\n",
    "username = getpass.getpass('Earthdata Username:')\n",
    "password = getpass.getpass('Earthdata Password:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AρρEEARS API URL\n",
    "API = 'https://lpdaacsvc.cr.usgs.gov/appeears/api' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll use the `requests` package to POST our username and password to the AρρEEARS system. A successful login will provide you with a token to be used later in this tutorial to submit a request. For more information or if you are experiencing difficulties, please see the [API Documentation](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#login)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_response = requests.post(f\"{API}/login\", auth=(username, password)).json()\n",
    "login_response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The response returns a Bearer Token which is needed to leverage the AρρEEARS API via HTTP request methods (e.g. POST and GET). Note that this token will expire approximately 48 hours after being acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the token to a variable\n",
    "token = login_response['token']\n",
    "head = {'Authorization': f\"Bearer {token}\"} \n",
    "head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Submit an Area Request <a id=\"submittask\"></a>\n",
    "The [Tasks](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#tasks) service, among other things (see below), is used to submit requests (e.g. POST and GET) to the AρρEEARS system. Each call to the [Tasks](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#tasks) service is associated with your user account. Therefore, each of the calls to this service require an authentication token. The [*submit task*](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#submit-task) API call provides a way to submit a new request. It accepts data via JSON, query string, or a combination of both. In the example below, we will compile a json and submit a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Import a shapefile  <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this example, we are interested in Yellowstone National Park. We will use the `Geopandas` package to import a shapefile that contains the adminstrative boundary for the park. The shapefile was extracted from the [National Park Service unit boundaries shapefile](https://irma.nps.gov/DataStore/DownloadFile/621132) distributed by [National Park Service - Land Resources Division](https://irma.nps.gov/DataStore/Reference/Profile/2224545?lnv=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellowstone = gpd.read_file('Data/yellowstone_subset_geo.shp')\n",
    "yellowstone.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Geopandas imports the shapefile in as a Geopandas GeoDataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(yellowstone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need to convert the `Geopandas GeoDataframe` into an object that has a  geojson structure. We'll use the method `json.loads` to make the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yellowstone = json.loads(yellowstone.to_json())\n",
    "#yellowstone\n",
    "type(yellowstone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **yellowstone** variable is now a python dictionary that matches the geojson structure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Compile the JSON payload to submit to AρρEEARS <a id=\"2.2\"></a>\n",
    "> Many of the required items needed in the AρρEEARS API request payload have multiple options. For example, AρρEEARS has several projections that can be selected for the output. We can use the AρρEEARS API to find out what projections are availables. In this example, we are explicitly assigning our projection to the **proj** variable. To find out how to use the AρρEEARS API to list the available options for each parameter, check out the [AρρEEARS API Tutorials](https://git.earthdata.nasa.gov/projects/LPDUR/repos/appeears-api-getting-started/browse) produced by the [LP DAAC](https://lpdaac.usgs.gov/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'Yellowstone_NP_Vegetation'    # User-defined name of the task\n",
    "task_type = 'area'                         # Type of task, area or point\n",
    "proj = 'geographic'                        # Set output projection \n",
    "outFormat = 'netcdf4'                      # Set output file format type\n",
    "startDate = '01-01-2016'                   # Start of the date range for which to extract data: MM-DD-YYYY\n",
    "endDate = '12-31-2018'                     # End of the date range for which to extract data: MM-DD-YYYY\n",
    "recurring = False                          # Specify True for a recurring date range\n",
    "#yearRange = [2000,2016]\n",
    "\n",
    "prodLayer = [{'layer': '_250m_16_days_NDVI', 'product': 'MOD13Q1.006'}]    # See layer names for MOD13Q1.006 here: https://lpdaacsvc.cr.usgs.gov/appeears/api/product/MOD13Q1.006\n",
    "#prodLayer = [{'layer': '_250_16_days_NDVI', 'product': 'MOD13Q1.006'}, {'layer': 'LC_Type1', 'product': 'MCD12Q1.006'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    'task_type': task_type,\n",
    "    'task_name': task_name,\n",
    "    'params': {\n",
    "         'dates': [\n",
    "         {\n",
    "             'startDate': startDate,\n",
    "             'endDate': endDate\n",
    "         }],\n",
    "         'layers': prodLayer,\n",
    "         'output': {\n",
    "                 'format': {\n",
    "                         'type': outFormat}, \n",
    "                         'projection': proj},\n",
    "         'geo': yellowstone,\n",
    "    }\n",
    "}\n",
    "#task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **task** object is what we will submit to the AρρEEARS system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Submit a task request <a id=\"2.3\"></a> \n",
    "> We will now submit our **task** object to AρρEEARS using the [*submit task*](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#submit-task) API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_response = requests.post(f\"{API}/task\", json=task, headers=head)    # Post json to the API task service, return response as json\n",
    "task_response.json()                                                     # Print task response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A task ID is generated for each request and is returned in the response. Task IDs are unique for each request and are used to check request status, explore request details, and list files generated for the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = task_response.json()['task_id']\n",
    "task_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Get task status <a id=\"2.4\"></a>\n",
    "> We can use the [Status](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#status) service to retrieve information on the status of all task requests that are currently being processed for your account. We will use the [*task status*](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#task-status) API call with our **task_id** to get information on the request we just submitted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status_response = requests.get(f\"{API}/status/{task_id}\", headers=head)\n",
    "status_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For longer running requests we can gently ping the API to get the status of our submitted request using the snippet below. Once the request is complete, we can move on to downloading our request contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starttime = time.time()\n",
    "#while requests.get(f\"{API}/task/{task_id}\", headers=head).json()['status'] != 'done':\n",
    "#    print(requests.get(f\"{API}/task/{task_id}\", headers=head).json()['status'])\n",
    "#    time.sleep(20.0 - ((time.time() - starttime) % 20.0))\n",
    "#print(requests.get(f\"{API}/task/{task_id}\", headers=head).json()['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download a Request <a id=\"downloadrequest\"></a>\n",
    "The [Bundle](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#bundle) service provides information about completed tasks (i.e. tasks that have a status of **done**). A bundle will be generated containing all of the files that were created as part of the task request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 List files associated with the request  <a id=\"3.1\"></a>\n",
    "> The [list files](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#list-files) API call lists all of the files contained in the bundle which are available for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = requests.get(f\"{API}/bundle/{task_id}\").json()    # Call API and return bundle contents for the task_id as json\n",
    "bundle                                                     # Print bundle contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Download files in a request <a id=\"3.2\"></a>\n",
    ">The [download file](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#download-file) API call gives us the information needed to download all, or a subset, of the files available for a request. Just as the task has a **task_id** to identify it, each file in the bundle will also have a unique **file_id** which should be used for any operation on that specific file. The `Content-Type` and `Content-Disposition` headers will be returned when accessing each file to give more details about the format of the file and the filename to be used when saving the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `bundle` variable we created has more information than we need to download the files. We will first create a python dictionary to hold the **file_id** and associated **file_name** for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = {}\n",
    "for f in bundle['files']: \n",
    "    files[f['file_id']] = f['file_name']    # Fill dictionary with file_id as keys and file_name as values\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we will download the files using the **file_id**s from the dictionary into an output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outDir = os.path.join(os.getcwd(), 'Outputs')    # When executing from local machine\n",
    "outDir = 'Outputs'                                # When using binder\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    download_response = requests.get(f\"{API}/bundle/{task_id}/{file}\", stream=True)                                   # Get a stream to the bundle file\n",
    "    filename = os.path.basename(cgi.parse_header(download_response.headers['Content-Disposition'])[1]['filename'])    # Parse the name from Content-Disposition header \n",
    "    filepath = os.path.join(outDir, filename)                                                                         # Create output file path\n",
    "    with open(filepath, 'wb') as file:                                                                                # Write file to dest dir\n",
    "        for data in download_response.iter_content(chunk_size=8192): \n",
    "            file.write(data)\n",
    "print(f\"Downloaded files can be found at: {outDir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here are the files we just downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore AρρEEARS Outputs <a id=\"explore\"></a>\n",
    "Now that we have downloaded all the files from our request, let's start to check out our data! In our AρρEEARS request, we set the output format to 'netcdf4'. As a result, we have only one data file to deal with. We will open the dataset as an `xarray Dataset` and start to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Open and explore data using [`xarray`](http://xarray.pydata.org/en/stable/) <a id=\"4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [`Xarray`](http://xarray.pydata.org/en/stable/) extends and combines much of the core functionality from both the Pandas library and Numpy, hence making it very good at handling multi-dimensional (N-dimensional) datasets that contain labels (e.g. variable names or dimension name). Let's open the netcdf file with our data as an xarray object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xarray.open_dataset('Outputs/MOD13Q1.006_250m_aid0001.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Xarray has two fundamental  data structures. A `Dataset` holds multiple variables that potentially share the same coordinates and global metadata for the file (see above). A `DataArray` contains a single multi-dimensional variable and its coordinates, attributes, and metadata. Data values can be pull out of the DataArray as a `numpy.ndarray` using the `values` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds['_250m_16_days_NDVI']\n",
    "type(ds['_250m_16_days_NDVI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds['_250m_16_days_NDVI'].values\n",
    "type(ds['_250m_16_days_NDVI'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also pull out information for each coordinate item (e.g. lat, lon, time). Here we pull out the *time* coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The `cftime.DatetimeJulian` format of the time coordinate is a little problematic for some plotting libraries and analysis routines. We are going to [convert the time coordinate](https://stackoverflow.com/questions/55786995/converting-cftime-datetimejulian-to-datetime) to the more useable datetime format `datetime64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatimeindex = ds.indexes['time'].to_datetimeindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds['time'] = datatimeindex\n",
    "ds['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the data is in an xarray we can intuitively slice or reduce dataset. Let's select a single time slice from the normalized difference vegetation index (NDVI) variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['_250m_16_days_NDVI'].sel(time='2015-12-19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's pull out the NDVI DataArray from the Dataset and name the variable ndvi. This will make plotting a little easier later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi = ds['_250m_16_days_NDVI']\n",
    "ndvi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the the our DataArray still has all of it's associated attributes and metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Create summary statistics <a id=\"4.2\"></a>\n",
    "> The download bundle for each AρρEEARS request includes a CSV with summary statistics. Since we already have the data in our python environment lets calculate our own summary statistics and plot them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's calculate the mean, standard deviation, maximum value, and minimum value for each time interval in our DataArray creating a seperate variable for each statistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_mean = ds['_250m_16_days_NDVI'].mean(('lat', 'lon'))\n",
    "ndvi_sd = ds['_250m_16_days_NDVI'].std(('lat', 'lon'))\n",
    "ndvi_max = ds['_250m_16_days_NDVI'].max(('lat', 'lon'))\n",
    "ndvi_min = ds['_250m_16_days_NDVI'].min(('lat', 'lon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Create plots <a id=\"4.3\"></a>\n",
    "> We now have the `mean` and `standard deviation` for each time slice as well as the `maximum` and `minimum` values. Let's do some plotting! We will use the [`hvPlot`](https://hvplot.pyviz.org/index.html) package to create simple but interactive chart/plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_mean.hvplot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = (\n",
    "    ndvi_mean.hvplot.line(height=350, width=450, line_width=1.50, color='red', grid=True, padding=0.05) + \n",
    "    ndvi_sd.hvplot.line(height=350, width=450, line_width=1.50, color='red', grid=True, padding=0.05) + \n",
    "    ndvi_max.hvplot.line(height=350, width=450, line_width=1.50, color='red', grid=True, padding=0.05) + \n",
    "    ndvi_min.hvplot.line(height=350, width=450, line_width=1.50, color='red', grid=True, padding=0.05)\n",
    ").cols(2)\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(ndvi_mean, ndvi_sd, ndvi_max, ndvi_min) # Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's take a look at out ndvi variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ndvi.hvplot.line()\n",
    "ndvi.hvplot.line('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's create some box and whisker plots! Notice how we can use *time* to slice the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single date\n",
    "ndvi.sel(time='2016-05-08').hvplot.box('_250m_16_days_NDVI', by=['time'], rot=45, box_fill_color='lightblue', padding=0.1, width=450, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations between months\n",
    "ndvi.sel(time=slice('2016-05', '2016-10')).hvplot.box('_250m_16_days_NDVI', by=['time'], rot=45, box_fill_color='lightblue', padding=0.1, width=800, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obervations for the specified year\n",
    "ndvi.sel(time='2016').hvplot.box('_250m_16_days_NDVI', by=['time'], rot=45, box_fill_color='lightblue', padding=0.1, width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See if the trend fits with what the AρρEEARS interface provides. Paste the string below into your browser, **without** the `'` to make the comparison. They should match...hopefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"https://lpdaacsvc.cr.usgs.gov/appeears/view/{task_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now let's create a multidimensional (t,x,y) plot of our gridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.hvplot(groupby='time', cmap='BrBG', width=640, height=469, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Quality Filtering <a id=\"qualityfiltering\"></a>\n",
    "When available, AρρEEARS extracts and returns quality assurance (QA) data for each data file returned regardless of whether the user requests it. This is done to ensure that the user possesses the information needed to determine the usability and usefulness of the data they get from AρρEEARS. The [Quality](https://lpdaacsvc.cr.usgs.gov/appeears/api/#quality) service from the AρρEEARS API can be leveraged to create masks that filter out undesirable data values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that the xarray Dataset contains a data array/variable called `_250m_16_days_VI_Quality`, which has the same dimensions as the `_250m_16_days_NDVI` data array/variable. We can use the quality array to create a mask of poor-quality data. We'll use the [Quality](https://lpdaacsvc.cr.usgs.gov/appeears/api/?language=Python%203#quality) service to decode the quality assurance information. \n",
    "\n",
    "> We'll use the following criteria to mask out poor quality data:\n",
    "- high aerosol content\n",
    "- cloud contamination\n",
    "- snow and ice cover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Decode quality values <a id=\"5.1\"></a>\n",
    "> We do not want to decode the same value multiple times. Let's extract all of the unique data values from the `_250m_16_days_VI_Quality` xarray DataArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quality_values = pd.DataFrame(np.unique(ds._250m_16_days_VI_Quality.values), columns=['value']).dropna()\n",
    "quality_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following function decodes the data values from the `_250m_16_days_VI_Quality` xarray DataArray using the [Quality](https://lpdaacsvc.cr.usgs.gov/appeears/api/#quality) service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qualityDecode(qualityservice_url, product, qualitylayer, value):\n",
    "    req = requests.get(f\"{qualityservice_url}/{product}/{qualitylayer}/{value}\")\n",
    "    return(req.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we will create an empty dataframe to store the decoded quality information for the masking criteria we identified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_desc = pd.DataFrame(columns=['value', 'AQ_bits', 'AQ_description', 'MC_bits', 'MC_description', 'SI_bits', 'SI_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The for loop below goes through all of the unique quality data values, decodes them using the quality service, and appends the quality descriptions to our empty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in quality_values.iterrows():\n",
    "    decode_int = qualityDecode('https://lpdaacsvc.cr.usgs.gov/appeears/api/quality',\n",
    "                               'MOD13Q1.006',\n",
    "                               '_250m_16_days_VI_Quality',\n",
    "                               str(int(row['value'])))\n",
    "    quality_info = decode_int\n",
    "    df = pd.DataFrame({'value': int(row['value']),\n",
    "                       'AQ_bits': quality_info['Aerosol Quantity']['bits'], \n",
    "                       'AQ_description': quality_info['Aerosol Quantity']['description'], \n",
    "                       'MC_bits': quality_info['Mixed Clouds']['bits'],\n",
    "                       'MC_description': quality_info['Mixed Clouds']['description'],\n",
    "                       'SI_bits': quality_info['Possible snow/ice']['bits'],\n",
    "                       'SI_description': quality_info['Possible snow/ice']['description']}, index=[index])\n",
    "\n",
    "    quality_desc = quality_desc.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quality_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Create and apply quality mask <a id=\"5.2\"></a>\n",
    "> Now we have a dataframe with all of the quality information we need to create a quality mask. Next we'll identify the quality categories that we would like to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask_values = quality_desc[((quality_desc['AQ_description'] == 'Low')|\n",
    "                           (quality_desc['AQ_description'] == 'Average'))&\n",
    "                           (quality_desc['MC_description'] == 'No')&\n",
    "                           (quality_desc['SI_description'] == 'No')]\n",
    "mask_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's apply the mask to our xarray dataset, keeping only the values that we have deemed acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_masked = ds.where(ds['_250m_16_days_VI_Quality'].isin(mask_values['value']))\n",
    "ds_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Plot quality filtered data <a id=\"5.3\"></a>\n",
    "> Using the same plotting functionality from above, let's see how our data looks when we mask out the undesirable pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_masked['_250m_16_days_NDVI'].hvplot(groupby='time', cmap='BrBG', width=640, height=469, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Whoa! Looks like a lot of the pixels over the winter months didn't make the cut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's use xarray's powerfull idexing method to pull out the 'summer months' (i.e. June, July, and August)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_masked_jja = ds_masked['_250m_16_days_NDVI'].sel(time=ds_masked['time.season']=='JJA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_masked_jja['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_masked_jja.hvplot(groupby='time', cmap='BrBG', width=640, height=469, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial provides a template to use for your own research workflows. Leveraging the AρρEEARS API for extracting and formatting analysis ready data, and importing it directly into Python means that you can keep your entire research workflow in a single software program, from start to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h1> Contact Information </h1>\n",
    "    <h3> Material written by Aaron Friesz$^{1}$ & Cole Krehbiel$^{1}$ </h3>\n",
    "    <ul>\n",
    "        <b>Contact:</b> LPDAAC@usgs.gov <br> \n",
    "        <b>Voice:</b> +1-605-594-6116 <br>\n",
    "        <b>Organization:</b> Land Processes Distributed Active Archive Center (LP DAAC) <br>\n",
    "        <b>Website:</b> https://lpdaac.usgs.gov/ <br>\n",
    "        <b>Date last modified:</b> 05-15-2019 <br>\n",
    "    </ul>\n",
    "\n",
    "$^{1}$Innovate! Inc., contractor to the U.S. Geological Survey, Earth Resources Observation and Science (EROS) Center, Sioux Falls, South Dakota, 57198-001, USA. Work performed under USGS contract G15PD00467 for LP DAAC$^{2}$.\n",
    "\n",
    "$^{2}$LP DAAC Work performed under NASA contract NNG14HH33I.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
